inxi -G
Graphics:
  Device-1: AMD Rembrandt [Radeon 680M] driver: amdgpu v: kernel
  Display: x11 server: X.Org v: 1.21.1.4 driver: X: loaded: amdgpu,ati
    unloaded: fbdev,modesetting,vesa gpu: amdgpu resolution: 1024x600~60Hz
  OpenGL:
    renderer: REMBRANDT (rembrandt LLVM 15.0.7 DRM 3.42 5.15.0-151-generic)
    v: 4.6 Mesa 23.2.1-1ubuntu3.1~22.04.3

sudo lshw -C display
[sudo] password for madhekar:          
  *-display                 
       description: VGA compatible controller
       product: Rembrandt [Radeon 680M]
       vendor: Advanced Micro Devices, Inc. [AMD/ATI]
       physical id: 0
       bus info: pci@0000:35:00.0
       logical name: /dev/fb0
       version: c7
       width: 64 bits
       clock: 33MHz
       capabilities: pm pciexpress msi msix vga_controller bus_master cap_list fb
       configuration: depth=32 driver=amdgpu latency=0 resolution=1024,600
       resources: iomemory:7f0-7ef iomemory:7f0-7ef irq:75 memory:7fe0000000-7fefffffff memory:7ff0000000-7ff01fffff ioport:f000(size=256) memory:dc700000-dc77ffff

lspci | grep VGA
35:00.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Rembrandt [Radeon 680M] (rev c7)

lspci -k | grep -EA3 'VGA|3D|Display'
35:00.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Rembrandt [Radeon 680M] (rev c7)
	Subsystem: Advanced Micro Devices, Inc. [AMD/ATI] Rembrandt [Radeon 680M]
	Kernel driver in use: amdgpu
	Kernel modules: amdgpu

Actually Radeon 680M and 780M are supported by the latest ROCm 6.0, 
what you need to do is to set HSA_OVERRIDE_GFX_VERSION=10.3.0 for 680M, 
and HSA_OVERRIDE_GFX_VERSION=11.0.0 for 780M.

It has been patched to the point that they don't work anymore, but there is a side project that keeps the original support.
https://github.com/likelovewant/ollama-for-amd/releases

After fighting with it for hours, I stumbled across it and everything was ticky boo afterwards. 
(Radeon 680M / gfx1035)


----

The NVIDIA RTX 4090 for machine learning tasks
The NVIDIA RTX 4090 is a powerful consumer-grade GPU that is also well-suited for machine learning and deep learning workloads. 
Here's why it's a popular choice:
Significant performance improvements: It features the Ada Lovelace architecture, 16,384 CUDA cores, 512 4th-generation Tensor Cores, and 24GB of GDDR6X VRAM. The RTX 4090 delivers significantly higher performance compared to the previous generation RTX 3090, with studies showing a 30-50% improvement in AI model training. It also offers high memory bandwidth of 1,008 GB/s.
Cost-effectiveness: While the RTX 4090 is a powerful card, it can be a more cost-effective option for certain machine learning tasks compared to more specialized data center GPUs like the NVIDIA A100 or A6000.
Strong performance for specific tasks: It excels in tasks such as training large language models (handling models with up to 6 billion parameters), computer vision, natural language processing, and other deep learning applications.
DLSS 3.0 support: This technology can indirectly benefit deep learning tasks by optimizing GPU usage and improving performance.
CUDA support: The RTX 4090 supports the latest version of NVIDIA's CUDA-X AI Library, which provides optimized algorithms for deep learning applications. 
Considerations for machine learning with an RTX 4090
Multi-GPU scaling: While the RTX 4090 does not support NVIDIA's NVLink for multi-GPU scaling, it utilizes PCIe 4.0, which offers sufficient bandwidth for linking multiple GPUs. Benchmarks suggest that multi-GPU training with two RTX 4090s can provide near linear scaling for many models, though some may exhibit slightly lower performance gains.
Memory capacity: While 24GB of GDDR6X VRAM is substantial, some extremely large language models or tasks with very large datasets might benefit from the greater memory capacity offered by GPUs like the NVIDIA A6000 (48GB).
Cooling requirements: The RTX 4090 can generate significant heat, especially in multi-GPU configurations, requiring adequate cooling solutions. Liquid cooling is often recommended to maintain optimal performance and prevent thermal throttling.
Professional features: It's worth noting that the RTX 4090 lacks some enterprise-grade features like ECC (Error Correcting Code) memory found in dedicated data center GPUs, which might be important for mission-critical applications. 
Alternatives to an RTX 4090 for machine learning
Depending on the specific needs and budget, other GPUs might be considered:
NVIDIA A100/H100: For highly demanding tasks and large-scale model training, these professional data center GPUs offer superior performance, memory capacity, and features like NVLink for efficient multi-GPU scaling.
NVIDIA A6000: Provides a balance of performance and features for professional and scientific computing, including a larger memory capacity (48GB) and NVLink support.
Cloud computing/GPU rental services: Services like Vast.ai and SimplePod offer on-demand access to GPUs like the RTX 4090, which can be a cost-effective and flexible alternative to purchasing expensive hardware. 
Ultimately, the best GPU for machine learning depends on factors like budget, the specific type and scale of machine learning tasks, and the importance of professional-grade features and scaling capabilities. 

sudo du -sh /*
apt list --installed
sudo find /var/log -type f -name "*.gz" -delete

du -sh * | sort -nr | head -n 10